---
title: "STATS 102b HW1"
author: "Joshua Susanto"
date: "2023-04-13"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
```

## Problem 1: 

Consider the correlation coefficient r between two random variables X and Y. Recall that it is a measure of linear association between X and Y and takes values in the interval [−1, 1]; i.e., r(X, Y) ∈ [−1, 1].


(a) Write code to simulate data from a bivariate normal distribution with mean vector µ = [0 0] and correlation matrix R
Hint: Use the mvrnorm function in the R library MASS

```{r}
mu <- c(0,0)
n <- 50
r <- 1
sigma <- matrix(c(1, r , r , 1), 2, 2)
mvrnorm(n, mu, sigma)
```


(b) Generate simulated data from a bivariate multivariate normal distribution
with mean vector µ = [00] and correlation matrix R for the following cases:
1. Sample size n ∈ {20, 50, 100, 200} and correlation coefficient r = 0

```{r}
set.seed(405568250)
n <- c(20, 50, 100, 200)
mu <- c(0, 0)
sigma <- matrix(c(1, 0, 0, 1), 2, 2)
par(mfrow = c(2,2))

for(i in n){
  plot(mvrnorm(i, mu, sigma), pch = 16,
         xlab = "X_1", ylab = "X_2", main = paste(i, "Bivariate Normal Draws: r = 0"))
  points(x = mu[1], y = mu[2], col = "orange", pch = 4, cex = 2)
}

```


2. Sample size n ∈ {20, 50, 100, 200} and correlation coefficient r = 0.5

```{r}
set.seed(405568250)
n <- c(20, 50, 100, 200)
mu <- c(0, 0)
sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
par(mfrow = c(2,2))

for(i in n){
  plot(mvrnorm(i, mu, sigma), pch = 16,
         xlab = "X_1", ylab = "X_2", main = paste(i, "Bivariate Normal Draws: r = 0.5"))
  points(x = mu[1], y = mu[2], col = "orange", pch = 4, cex = 2)
}
```


3. Sample size n ∈ {20, 50, 100, 200} and correlation coefficient r = 0.85

```{r}
set.seed(405568250)
n <- c(20, 50, 100, 200)
mu <- c(0, 0)
sigma <- matrix(c(1, 0.85, 0.85, 1), 2, 2)
par(mfrow = c(2,2))

for(i in n){
  plot(mvrnorm(i, mu, sigma), pch = 16,
         xlab = "X_1", ylab = "X_2", main = paste(i, "Bivariate Normal Draws: r = 0.85"))
  points(x = mu[1], y = mu[2], col = "orange", pch = 4, cex = 2)
}
```


(c) Obtain the bootstrap sampling distribution of the sample correlation coefficient rˆ for the three cases in part (b), for the following number of bootstrap replicates
B ∈ {200, 1000, 5000, 10000}.

```{r}
bootsampling <- function(x, boot.replicates = B){
  x = as.matrix(x)
  nx = nrow(x)
  bootsamples = replicate(boot.replicates, x[sample.int(nx, replace = TRUE), ])
}
```


```{r}
set.seed(405568250)

n = c(20, 50, 100, 200) # Size of a single sample
B = c(200, 1000, 5000, 10000) # Number of bootstrap replicates
r = c(0,0.5,0.85)


for (i in r) {
  for (j in n) {
    mu <- c(0, 0)
    sigma <- matrix(c(1, i, i, 1), 2, 2)
    x <- mvrnorm(j, mu, sigma)
    par(mfrow = c(2,2))
    for (k in B) {
      boot.samples = bootsampling(x, k)
      xbar = apply(boot.samples, 3, cor)
      hist(xbar[2,], breaks = 35, freq = F, cex.main = 0.6, main = paste (j, "Samples with", i, "Correlation Bootstrap Sampling Distribution: B =", k))
    } 
  }
}
```


Comment on the results; in particular how the bootstrap sampling distribution
behaves as a function of the sample size n, the number of bootstrap replicates B and
the value of the correlation coefficient r.


We can see from our sampling distributions that as n increases, the distribution tends to center closer to our observed correlation coefficient. Additionally, we see that an increasing number of bootstraps tends to have the shape of our distribution more closely converge to normal. The changing of R in our data changed where our sampling distributions tend to center around as well.


## Problem 2:
The data set “cats” can be obtain through the following R code:

```{r}
library(MASS)
library(tidyverse)
data(cats)
summary(cats)
```

It contains the body weight (Bwt) in kilograms and the (Hwt) in grams of 47 female and 97 male cats.


Part (a):
• Obtain the bootstrap sampling distribution of the difference of sample means for
body weight between female and male cats

```{r}
set.seed(405568250)
female_weight <- cats$Bwt[cats$Sex == "F"]
male_weight <- cats$Bwt[cats$Sex == "M"]
par(mfrow = c(2,2))

B <- c(200, 1000, 5000, 10000)

for (i in B) {
  boot_samples_male <- bootsampling(male_weight, i)
  boot_samples_female <- bootsampling(female_weight, i)
  mean_diff <- apply(boot_samples_male, 2, mean) - apply(boot_samples_female, 2, mean)
  hist(mean_diff, breaks = 35, freq = F, cex.main = 0.65, main = paste("Body Weight Mean Difference Bootstrap Sampling Distribution: B =", i))
}
```


• Obtain the bootstrap sampling distribution of the difference of sample means for
heart weight between female and male cats


```{r}
set.seed(405568250)
female_ht_weight <- cats$Hwt[cats$Sex == "F"]
male_ht_weight <- cats$Hwt[cats$Sex == "M"]
par(mfrow = c(2,2))

B <- c(200, 1000, 5000, 10000)

for (i in B) {
  boot_samples_male <- bootsampling(male_ht_weight, i)
  boot_samples_female <- bootsampling(female_ht_weight, i)
  mean_diff <- apply(boot_samples_male, 2, mean) - apply(boot_samples_female, 2, mean)
  hist(mean_diff, breaks = 35, freq = F, cex.main = 0.65, main = paste("Heart Weight Mean Difference Bootstrap Sampling Distribution: B =", i))
}
```


Explain how many bootstrap replicates you decided to use and comment on the
results.


In my response I decided to use (200, 1000, 5000, 10000) boot strap samples in increasing order. From our sampling distributions we can see that the greater the bootstrap sample number the more our distribution shape closely resembles a normal curve. This makes sense as a larger number of bootstraps tends to bring our distribution closer to an accurate depiction of our true sample distribution.


Part (b):


• Obtain the bootstrap sampling distribution of the t-statistic when testing for
mean differences for body weight between female and male cats


```{r}
var(female_weight)
var(male_weight)
```


From this we can see that there is a large difference in the variances of the female and male body weights and we can use the following formulation for the t-statistic:

$$t = \frac{\bar{X}-\bar{Y}}{\sqrt{\frac{s^2_X}{m}+\frac{s^2_Y}{m}}} $$


```{r}
set.seed(405568250)
B <- c(200, 1000, 5000, 10000)
par(mfrow = c(2,2))

for (i in B) {
  boot_samples_male <- bootsampling(male_weight, i)
  boot_samples_female <- bootsampling(female_weight, i)
  mean_diff <- apply(boot_samples_male, 2, mean) - apply(boot_samples_female, 2, mean)
  denom <- apply(boot_samples_male, 2, var)/length(male_weight) + apply(boot_samples_female, 2, var)/length(female_weight)
  t <- mean_diff/sqrt(denom)
  hist(t, breaks = 35, freq = F, cex.main = 0.75, main = paste("t Statistic Bootstrap Sampling Distribution: B =", i))
}
```


• Obtain the bootstrap sampling distribution of the t-statistics when testing for
mean differences for heart weight between female and male cats


```{r}
var(female_ht_weight)
var(male_ht_weight)
```


From this we can see that there is a large difference in the variances of the female and male heart weights and we can use the following formulation for the t-statistic:

$$t = \frac{\bar{X}-\bar{Y}}{\sqrt{\frac{s^2_X}{m}+\frac{s^2_Y}{m}}} $$


```{r}
set.seed(405568250)
par(mfrow = c(2,2))
B <- c(200, 1000, 5000, 10000)

for (i in B) {
  boot_samples_male <- bootsampling(male_ht_weight, i)
  boot_samples_female <- bootsampling(female_ht_weight, i)
  mean_diff <- apply(boot_samples_male, 2, mean) - apply(boot_samples_female, 2, mean)
  denom <- apply(boot_samples_male, 2, var)/length(male_ht_weight) + apply(boot_samples_female, 2, var)/length(female_ht_weight)
  t <- mean_diff/sqrt(denom)
  hist(t, breaks = 35, freq = F, cex.main = 0.75, main = paste("t Statistic Bootstrap Sampling Distribution: B =", i))
}
```


Explain how many bootstrap replicates you decided to use and comment on the
results.


In my response I decided to use (200, 1000, 5000, 10000) boot strap samples in increasing order. Similarly from previous bootstrap sampling distributions, we can see that the greater the bootstrap sample number the more our distribution shape closely resembles a normal curve. However, we can see that our t statistic sampling distribution seems to converge to normality a bit slower than our mean sampling distribution with increasing bootstrap samples. This is likely due to the mean sampling distributions being a relatively simple statistic that is usually well approximated by a normal curve whereas a t-statistic is much more complex and has much more room for variation. However, with a large number of bootstraps both of these statistics converge to normality with our data.



Part (c):
Using your code from Problem 1(c):
• Obtain the bootstrap sampling distribution of the sample correlation coefficient
between body weight and heart weight for female cats 

```{r}
female_cats <- cats %>% filter(Sex == 'F') %>% select(Bwt, Hwt) %>% data.matrix()
par(mfrow = c(2,2))
B <- c(200, 1000, 5000, 10000)

for (i in B) {
  boot_samples_female <- bootsampling(female_cats, i)
  female_corr <- apply(boot_samples_female, 3, cor)
  hist(female_corr[2,], breaks = 35, freq = F, cex.main = 0.6, main = paste("Female Cats Correlation Coefficient Bootstrap Sampling Distribution: B =", i))
}
```


• Obtain the bootstrap sampling distribution of the sample correlation coefficient
between body weight and heart weight for male cats


```{r}
male_cats <- cats %>% filter(Sex == 'M') %>% select(Bwt, Hwt) %>% data.matrix()
par(mfrow = c(2,2))
B <- c(200, 1000, 5000, 10000)

for (i in B) {
  boot_samples_male <- bootsampling(male_cats, i)
  male_corr <- apply(boot_samples_male, 3, cor)
  hist(male_corr[2,], breaks = 35, freq = F, cex.main = 0.6, main = paste("Male Cats Correlation Coefficient Bootstrap Sampling Distribution: B =", i))
}
```


Explain how many bootstrap replicates you decided to use and comment on the results.


In my response I decided to use (200, 1000, 5000, 10000) boot strap samples in increasing order. Similarly from previous bootstrap sampling distributions, we can see that the greater the bootstrap sample number the more our distribution shape closely resembles a normal curve. In terms of convergence, we see a similar slower convergence with the t statistic sampling distribution, perhaps because correlation coefficient is also a complex statistic. We see that our distribution converges to center around 0.8 - 0.81 which is a good indication that the true correlation coefficient of our data is around that point. We see some slight skewness to the left in our data as well but other than that the curve remains relatively stable and smooth.

